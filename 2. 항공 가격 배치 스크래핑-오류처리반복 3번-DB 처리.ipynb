{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 항공가격 배치 스크래핑\n",
    "- 크롤링 배치 처리 파일 스크래핑 처리\n",
    "    * 전체 파일 스크래핑 처리\n",
    "    * 미처리 파일 오류 처리 3회 실시\n",
    "    * 스크래핑 처리 파일 DB 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import sqlite3\n",
    "import csv, io, os\n",
    "from common.env_variable import *\n",
    "from common.batch_util import *\n",
    "from common.scrap_func import *\n",
    "from common.crawl_func import *\n",
    "from common.log_util import *\n",
    "from common.util import save_raw_data\n",
    "## 로그 초기화\n",
    "logger_initialize('scrap_logger_setting.json')\n",
    "init_env_variable('common/env_variable.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 국내선 국제선 구분하여 폴더에 있는 파일 scraping 처리\n",
    "## 포맷 : 대상항공사코드,플라이트,출발일,출발지,도착지,출발시간,도착시간,가격,유류세,세금,좌석수\n",
    "## 오류 발생한 파일들은 해당 폴더에 그대로 방치, 주기적으로 체크하여 오래 남아 있는 파일들은 문제있는 파일로 취급 하여 재처리\n",
    "def batch_scrap(crawl_dir,scrap_data_dir,dom_int=None):\n",
    "    start_time = datetime.today()\n",
    "    log_msgs = ['start batch job','scraping crawled file']\n",
    "    log(log_msgs,level=logging.INFO)\n",
    "    scrap_cnt = 0\n",
    "    ## 스크랩 대상 파일(파일명, 내용)\n",
    "    crawl_dir = crawl_dir\n",
    "    files = get_crawled_file_list(crawl_dir)\n",
    "    for file in files:\n",
    "        log(file,level=logging.DEBUG)\n",
    "        scrap_file = crawl_dir+'/'+file\n",
    "        head, raw_data = read_crawled_file(scrap_file)\n",
    "        ## 헤더 정보 읽어오기\n",
    "        #head = get_headinfo(raw_data.splitlines()[0])\n",
    "        log([head['dom_int'],head['site_code']],level=logging.DEBUG)\n",
    "        if dom_int is not None and head['dom_int'] != dom_int: ## 국내선/국제선 체크\n",
    "            continue\n",
    "        site_code = head['site_code']\n",
    "        ## 해당 함수 생성\n",
    "        func = get_scrap_site_func(head['dom_int'],head['site_code'])\n",
    "        log(['check func',func],level=logging.DEBUG)\n",
    "        if func is None: ## 해당 함수가 없을 경우 다음으로\n",
    "            continue\n",
    "        try:\n",
    "            scraped_list = func(head,raw_data)#''.join(raw_data.splitlines()[1:])))\n",
    "        except:\n",
    "            log_msgs = ['Error occured when doing scrap func -',func]\n",
    "            log(log_msgs,level=logging.ERROR)\n",
    "            continue\n",
    "            \n",
    "        ## 스크래핑 실패시 해당 파일 폴더에 보관, 해당 폴더 모니터링시 장기 미처리 파일 체크하여 처리\n",
    "        if scraped_list is None:\n",
    "            ## 출렬 처리\n",
    "            log('Error occured in {}!'.format(file),level=logging.DEBUG)\n",
    "            continue\n",
    "        ## CSV 처리\n",
    "        scraped_list_to_csv(set_headinfo(**head),scraped_list,scrap_data_dir+\"/\"+file.split(\".\")[0]+\".csv\")\n",
    "        ## 처리 파일 이동 처리\n",
    "        ## 이동 처리시 이미 파일이 존재하는 경우 삭제후 처리\n",
    "        if os.path.exists(os.path.join(SCRAP_OK_DIR,os.path.split(scrap_file)[-1])):\n",
    "            os.remove(os.path.join(SCRAP_OK_DIR,os.path.split(scrap_file)[-1]))\n",
    "        move_scraped_file(scrap_file,SCRAP_OK_DIR)\n",
    "        scrap_cnt += 1\n",
    "\n",
    "    end_time = datetime.today()\n",
    "    log_msgs = ['end batch job','scraping crawled file',\n",
    "                'elapsed -{}'.format(end_time-start_time),'Total crawl:{} files saved.'.format(scrap_cnt)]\n",
    "    log(log_msgs,level=logging.INFO)\n",
    "## 에러 파일 추가 크롤링 처리\n",
    "def batch_error(crawl_dir):\n",
    "    ## 에러 파일 추가 크롤링 처리\n",
    "    log('start recrawling batch job for error files')\n",
    "    files = get_crawled_file_list(crawl_dir)\n",
    "    cnt = 0\n",
    "    for file in files:\n",
    "        log(['error file',file])\n",
    "        head,_=read_crawled_file(crawl_dir+'/'+file)\n",
    "        func,isairline=get_crawl_site_func(head['dom_int'],head['site_code'])\n",
    "        if isairline:\n",
    "            raw_data = crawling_func(func,head['dpt'],head['arr'],head['dpt_date'])\n",
    "            new_file = file_name(head['site_code'],head['dpt'],head['arr'],head['dpt_date'])\n",
    "            new_head = set_headinfo(head['site_code'],head['dom_int'],head['site_code'],head['dpt'],head['arr'],\n",
    "                                head['dpt_date'],crawl_date=head['crawl_date'])\n",
    "        else:\n",
    "            raw_data = crawling_func(func,head['airline'],head['dpt'],head['arr'],head['dpt_date'])\n",
    "            new_file = file_name(head['site_code'],head['dpt'],head['arr'],head['dpt_date'],airline=head['airline'])\n",
    "            new_head = set_headinfo(head['site_code'],head['dom_int'],head['airline'],head['dpt'],head['arr'],\n",
    "                                head['dpt_date'],crawl_date=head['crawl_date'])\n",
    "        ## 처리 파일 이동 : 해당 폴더의 error 폴더로 이동 처리\n",
    "        ## 파일 명이 중복되는 경우 발생 가능, 이동 처리후 저장\n",
    "        move_scraped_file(crawl_dir+'/'+file,crawl_dir+'/error')\n",
    "        save_raw_data(new_file,raw_data,head=new_head)\n",
    "        cnt += 1\n",
    "    log(['end recrawling batch job','total {} files saved!'.format(cnt)])\n",
    "## 파일 배치 처리\n",
    "def batch_db(scrap_dir,db_ok_dir,db):\n",
    "    start_time = datetime.today()\n",
    "    log_msgs = ['start batch job','doing db job with scraped data file']\n",
    "    log(log_msgs,level=logging.INFO)\n",
    "    ## 파일 리스트 생성\n",
    "    db_cnt = 0\n",
    "    files = get_crawled_file_list(scrap_dir)\n",
    "    ## 각 파일 처리\n",
    "    for file in files:\n",
    "        log(file,level=logging.DEBUG)\n",
    "        csv_file = scrap_dir+'/'+file\n",
    "        head, raw_data = read_crawled_file(csv_file,csv=True)\n",
    "        ## 스키마 체크 - 차후 오류 로그 체크하여 해당 내용 확인후 반영\n",
    "        ## DB 처리\n",
    "        log('execute insert query',level=logging.DEBUG)\n",
    "        i_cnt = scraped_csv_to_db(head,raw_data,db)\n",
    "        log(['insert result',i_cnt],level=logging.DEBUG)\n",
    "        db_cnt += i_cnt\n",
    "        ## 정상 처리 파일 처리\n",
    "        if i_cnt > 0:\n",
    "            move_scraped_file(csv_file,db_ok_dir)\n",
    "    end_time = datetime.today()\n",
    "    log_msgs = ['end batch job','doing db job with scraped data file',\n",
    "                'elapsed -{}'.format(end_time-start_time),'Total :{} rows inserted.'.format(db_cnt)]\n",
    "    log(log_msgs,level=logging.INFO)\n",
    "    \n",
    "def scraped_csv_to_db(head,raw_data,db):\n",
    "    log('$$ check csv to db process.',level=logging.INFO)\n",
    "    ## CSV 파일 읽기\n",
    "    csv_data = csv.reader(io.StringIO(raw_data))\n",
    "    ## DB 처리 리스트 생성\n",
    "    target_list = []\n",
    "    for d in csv_data:\n",
    "        ## scrap_date, scrap_site, patten(1 편도)\n",
    "        td_list = [head['crawl_date'],head['site_code'],'1']\n",
    "        ## airline,flt,dpt,arr,dpt_time,arr_time,fare,tax1,tax2,seat\n",
    "        td_list.extend(d)\n",
    "        target_list.append(td_list)\n",
    "    ## DB 처리\n",
    "    cnt = 0\n",
    "    conn = sqlite3.connect(db)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        sql = \"insert into airfare_scraped_data\"+\\\n",
    "        \"(scrap_date,scrap_site,patten,airline,flt,dpt_date,dpt,arr,dpt_time,arr_time,fare,tax1,tax2,seat)\"+\\\n",
    "        \"values(?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\n",
    "        cur.executemany(sql,target_list)\n",
    "        conn.commit()\n",
    "        cnt = len(target_list)\n",
    "    except sqlite3.Error as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        log(e,level=logging.ERROR)\n",
    "        cnt = 0\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    log('$$ check db process result : {}'.format(cnt),level=logging.INFO)\n",
    "    return cnt\n",
    "\n",
    "## 스크랩 처리후 남은 파일 처리 - 필요없는 파일 nodata 폴더로 이동\n",
    "## csv 파일 삭제 처리\n",
    "def move_nodata_files():\n",
    "    src = CRAWL_DIR\n",
    "    dst = NODATA_DIR\n",
    "    print('start moving error files')\n",
    "    nodata_files = get_files(src,check='scrap')\n",
    "    print('nodata files : ',len(nodata_files))\n",
    "    for f in nodata_files:\n",
    "        move_file(os.path.join('crawl', f),dst)\n",
    "    print('end moving error files')\n",
    "    ## csv 파일 데이터 삭제 처리\n",
    "    for file in get_files(SCRAP_DATA_DIR):\n",
    "        os.remove('scrap_data/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dom_int = None # 0 - 국내선, 1 - 국제선, None - 모두\n",
    "batch_scrap(CRAWL_DIR,SCRAP_DATA_DIR,dom_int=dom_int)\n",
    "\n",
    "batch_cnt = 3\n",
    "db_file = 'airfare_scraped_data.db'\n",
    "for i in range(batch_cnt):\n",
    "    batch_error(CRAWL_DIR)\n",
    "    batch_scrap(CRAWL_DIR,SCRAP_DATA_DIR,dom_int=dom_int)\n",
    "batch_db(SCRAP_DATA_DIR,DB_OK_DIR,db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 추가 오류 처리 진행시\n",
    "dom_int = None\n",
    "db_file = 'airfare_scraped_data.db'\n",
    "batch_error(CRAWL_DIR)\n",
    "batch_scrap(CRAWL_DIR,SCRAP_DATA_DIR,dom_int=dom_int)\n",
    "batch_db(SCRAP_DATA_DIR,DB_OK_DIR,db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## 스크랩 잔존 파일 이동 처리, CSV 파일 삭제\n",
    "move_nodata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "db_file = 'airfare_scraped_data.db'\n",
    "batch_db(SCRAP_DATA_DIR,DB_OK_DIR,db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
